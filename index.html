<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiang (Ryan) Li</title>
  
  <meta name="author" content="Xiang Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- Removing animate.css which causes the rising effect -->
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"> -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">

  <!-- Favicon –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="apple-touch-icon" sizes="57x57" href="fav/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="fav/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="fav/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="fav/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="fav/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="fav/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="fav/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="fav/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="fav/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="fav/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="fav/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="fav/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="fav/favicon-16x16.png">
  <link rel="manifest" href="fav/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="fav/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  <base target="_blank">
  
</head>

<body>
  <div class="main-container">
    <table style="width:100%;max-width:1200px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:70%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Xiang (Ryan) Li</name>
                </p>
                <p> My name is Xiang Li (李想; pronounced "Shiang Li"). I am a fifth-year Ph.D. student at the University of Illinois Urbana–Champaign (UIUC), advised by <a href="https://rehg.org/">Prof. James M. Rehg</a>. My research focuses on the analysis and alignment of visual generative AI, with an emphasis on 3D generation. </p>
                <p> In recent years, I have interned at Meta Superintelligence Lab, working with <a href="https://sites.google.com/view/weiyaowang/home">Weiyao Wang</a>, <a href="https://alexsax.github.io/">Sasha Sax</a>, <a href="https://scholar.google.com/citations?user=XY6Nh9YAAAAJ&hl=en">Hao Tang</a>, and <a href="https://scholar.google.com/citations?user=A-wA73gAAAAJ&hl=en">Matt Feiszli</a>. I also interned at Google Research with <a href="https://boqinggong.github.io/">Boqing Gong</a>. I received my bachelor's degree from The Hong Kong University of Science and Technology (HKUST), where I was advised by <a href="https://yuwingtai.github.io/">Prof. Yu-Wing Tai</a> and <a href="https://scholar.google.com/citations?user=EWfpM74AAAAJ">Prof. Chi-Keung Tang</a>. </p>
                <p style="text-align:center" class="social-links">
                  <a href="mailto:xiangl12@illinois.edu"><i class="fas fa-envelope"></i> Email</a> &nbsp/&nbsp
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                  <a href="https://scholar.google.com/citations?user=3Ds7hOQAAAAJ"><i class="fas fa-graduation-cap"></i> Google Scholar</a> &nbsp/&nbsp
                  <!-- <a href="Resume_Xiang_Li_UIUC_2025_intern.pdf"><i class="fas fa-file-alt"></i> CV</a> &nbsp/&nbsp -->
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                  <!-- <a href="https://github.com/RyanXLi/"><i class="fab fa-github"></i> Github</a> &nbsp/&nbsp -->
                  <a href="https://www.linkedin.com/in/xiang-li-52aa4912a"><i class="fab fa-linkedin"></i> LinkedIn</a>
                  
                </p>
              </td>
              <td style="padding:2.5%;width:30%;max-width:30%">
                <!-- <div style="padding: 20px;"> -->
                <img style="width:85%;max-width:85%;border-radius: 50%;" alt="profile photo" src="images/xiang_cropped.jpeg">
                <!-- </div> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Selected Research</heading>
                <!-- <p>
                  I'm interested in computer vision, machine learning, optimization, and image processing.
                  Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images.
                  Representative papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <!-- Publication item 1 -->
            <tr class="publication-item">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div>
                  <a href="https://ai.meta.com/sam3d/">
                    <img src="./images/sam3d.png"
                    alt="Figure in sam3d paper"
                    class="publogo img-fluid float-left rounded g" width="100%" a=""
                    />
                  </a>
                </div>
              </td>
              
              <td style="padding:20px;width:70%;vertical-align:middle">
                <a href="https://ai.meta.com/sam3d/">
                <papertitle>SAM 3D: 3Dfy Anything in Images</papertitle>
                </a>
                <br>
                <span><strong>SAM 3D Team</strong></span>
                <br>
                <em>Tech Report</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2511.16624">paper</a>
                /
                <a href="https://ai.meta.com/sam3d/">project page</a>
                /
                <a href="https://github.com/facebookresearch/sam-3d-objects">code</a>
                <p></p>
                <p>We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting
                  geometry, texture, and layout from a single image. <br>
                <strong>Personal Contribution</strong>: Main contributor to post-training, preference optimization.</p>
              </td>
            </tr>


            <!-- Publication item 1 -->
            <tr class="publication-item">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div>
                  <a href="./cue3d/index.html">
                    <img src="./cue3d/static/assets/cue3d.png"
                    alt="Figure in cue3d paper"
                    class="publogo img-fluid float-left rounded g" width="85%" a=""
                    />
                  </a>
                </div>
              </td>
              
              <td style="padding:20px;width:70%;vertical-align:middle">
                <a href="./cue3d/index.html">
                <papertitle>Cue3D: Quantifying the Role of Image Cues in Single-Image 3D Generation</papertitle>
                </a>
                <br>
                <span><strong>Xiang Li*</strong></span>, 
                <a href="https://ziruiwang409.github.io/">Zirui Wang*</a>, 
                <a href="https://zixuanh.com/">Zixuan Huang</a>, 
                <a href="https://rehg.org/">James M. Rehg</a>
                <br>
                <em>In NeurIPS</em>, 2025 (Spotlight ✨)
                <br>
                <a href="./cue3d/static/assets/cue3d.pdf">paper</a>
                /
                <a href="./cue3d/index.html">project page</a>
                <p></p>
                <p>We introduce Cue3D, the first comprehensive, model-agnostic framework for quantifying the influence of individual image cues in single-image 3D generation.</p>
              </td>
            </tr>

            <!-- Publication item 2 -->
            <tr class="publication-item">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div>
                  <!-- <img src="images\vscos.png"
                    alt="Figure in VSCOS paper"
                    class="publogo img-fluid float-left rounded g" width="100%" a=""
                    /> -->
                    <a href="./reflect3d/index.html">
                    <video id="teaser" autoplay muted loop playsinline width="100%">
                      <source src="./images/ssb_video_720p_2x2.mp4"
                              type="video/mp4">
                    </video>
                    </a>
                </div>
              </td>

              
              
              <td style="padding:20px;width:70%;vertical-align:middle">
                <a href="./reflect3d/index.html">
                <papertitle>Symmetry Strikes Back: From Single-Image Symmetry Detection to 3D Generation</papertitle>
                </a>
                <br>
                <span><strong>Xiang Li</strong></span>, 
                <a href="https://zixuanh.com/">Zixuan Huang</a>, 
                <a href="https://anhthai1997.com/">Anh Thai</a>, 
                <a href="https://rehg.org/">James M. Rehg</a>
                <br>
                <em>In CVPR</em>, 2025 (Highlight ✨)
                <br>
                <!-- <a href="oneshot/index.html">project page</a>
                / -->
                <a href="./reflect3d/static/assets/reflect3d.pdf">paper</a>
                /
                <a href="./reflect3d/index.html">project page</a>
                <p></p>
                <p>We propose Reflect3D, a zero-shot single-image 3D reflection symmetry detector; we improve single-image 3D generation through symmetry-aware optimization. </p>
              </td>
            </tr>

            <!-- Publication item 3 -->
            <tr class="publication-item">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div>
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Video_State-Changing_Object_Segmentation_ICCV_2023_paper.pdf">
                  <img src="images\vscos.png"
                    alt="Figure in VSCOS paper"
                    class="publogo img-fluid float-left rounded g" width="100%" a=""
                    />
                  </a>
                </div>
              </td>
              
              <td style="padding:20px;width:70%;vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Video_State-Changing_Object_Segmentation_ICCV_2023_paper.pdf">
                <papertitle>Video State-Changing Object Segmentation</papertitle>
                </a>
                <br>
                <a href="https://www.linkedin.com/in/venom-yu/">Jiangwei Yu*</a>, 
                <strong>Xiang Li*</strong>, 
                <a href="https://colinzhaoust.github.io/">Xinran Zhao</a>, 
                <a href="https://panda0881.github.io/Hongming_Homepage/">Hongming Zhang</a>, 
                <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
                <br>
                <em>In ICCV</em>, 2023
                <br>
                <!-- <a href="oneshot/index.html">project page</a>
                / -->
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Video_State-Changing_Object_Segmentation_ICCV_2023_paper.pdf">paper</a>
                /
                <a href="https://venom12138.github.io/VSCOS.github.io/">project page</a>
                /
                <a href="https://github.com/venom12138/vscos">dataset and code</a>
                <p></p>
                <p>We present a weakly-supervised Video State-Changing Object Segmentation (VSCOS) benchmark, revealing challenges in current VOS models for state-changing objects and introducing three solutions for improved state-changing object segmentation. </p>
              </td>
            </tr>

            <!-- Publication item 4 -->
            <tr class="publication-item">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div>
                  <a href="https://openreview.net/pdf?id=AIeeXKsspI">
                  <img src="images\youtubepd.png"
                    alt="Figure in YoutubePD paper"
                    class="publogo img-fluid float-left rounded g" width="100%" a=""
                    />
                  </a>
                </div>
              </td>
              
              <td style="padding:20px;width:70%;vertical-align:middle">
                <a href="https://openreview.net/pdf?id=AIeeXKsspI">
                <papertitle>YouTubePD: A Multimodal Benchmark for Parkinson's Disease Analysis</papertitle>
                </a>
                <br>
                <a href="https://www.linkedin.com/in/andy-zhou-679376206/">Andy Zhou*</a>,
                <a href="https://www.linkedin.com/in/samuelwli/">Samuel Li*</a>,
                <a href="https://psriram4.github.io/">Pranav Sriram*</a>,
                <strong>Xiang Li*</strong>, 
                <a href="https://www.linkedin.com/in/jiahua-dong-190431268/">Jiahua Dong*</a>, 
                <a href="https://www.linkedin.com/in/anshgs/">Ansh Sharma</a>, 
                <a href="https://scholar.google.com/citations?user=PtmjwooAAAAJ&hl=en">Yuanyi Zhong</a>,
                <a href="https://scholar.google.com/citations?user=h0YX9akAAAAJ&hl=en">Shirui Luo</a>,
                <a href="https://www.linkedin.com/in/maria-jaromin-64207936/">Maria Jaromin</a>,
                <a href="https://users.ncsa.illinois.edu/kindr/">Volodymyr Kindratenko</a>,
                <a href="https://healtheng.illinois.edu/people/georgeheintz">George Heintz</a>,
                <a href="https://www2.osfhealthcare.org/providers/christopher-m-zallek-1465296">Christopher Zallek</a>,
                <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
                <br>
                <em>In NeurIPS Datasets and Benchmarks Track</em>, 2023
                <br>
                <!-- <a href="oneshot/index.html">project page</a> -->
                <!-- / -->
                <a href="https://openreview.net/pdf?id=AIeeXKsspI">paper</a>
                /
                <a href=" https://uiuc-yuxiong-lab.github.io/YouTubePD/">project page</a>
                /
                <a href="https://github.com/samwli/YouTubePD-data">dataset</a>  
                <p></p>
                <p>We introduce YouTubePD, the first public multimodal benchmark for Parkinson's Disease (PD) analysis, crowdsourced from existing YouTube videos featuring over 200 subjects.</p>
                <!-- The benchmark provides diverse expert annotations and suggests three tasks, with experimental evaluations indicating the potential and limits of deep learning models for real-world clinical applications. -->
              </td>
            </tr>

            <!-- Publication item 5 -->
            <tr class="publication-item">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div>
                  <a href="https://arxiv.org/abs/2005.03819">
                  <img src="images/1_oneshot.jpeg"
                    alt="Figure in Oneshot paper"
                    class="publogo img-fluid float-left rounded g" width="100%" a=""
                    />
                  </a>
                </div>
              </td>
              
              <td style="padding:20px;width:70%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2005.03819">
                <papertitle>One-Shot Object Detection without Fine-Tuning</papertitle>
                </a>
                <br>
                <strong>Xiang Li*</strong>, 
                <a href="https://lzhangbj.github.io/">Lin Zhang*</a>, 
                <span>Yau Pun Chen</span>, 
                <a href="https://scholar.google.com/citations?user=nFhLmFkAAAAJ">Yu-Wing Tai</a>, 
                <a href="http://www.cs.ust.hk/~cktang/bio-sketch-review.htm">Chi-Keung Tang</a>
                <br>
                <em>arXiv</em>, 2020
                <br>
                <a href="https://arxiv.org/abs/2005.03819">paper</a>
                /
                <a href="oneshot/index.html">project page</a>
                /
                <a href="https://github.com/RyanXLi/OneshotDet">code</a>
                <p></p>
                <p>We introduce a two-stage model and training strategies for one-shot object detection by integrating the metric learning with an anchor-free Faster R-CNN-style detection pipeline.</p>
              </td>
            </tr>

            <!-- Publication item 6 -->
            <tr class="publication-item">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div>
                  <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_FSS-1000_A_1000-Class_Dataset_for_Few-Shot_Segmentation_CVPR_2020_paper.pdf">
                  <img src="images\0_fss.jpeg"
                    alt="Figure in FSS paper"
                    class="publogo img-fluid float-left rounded g" width="100%" a=""
                    />
                  </a>
                </div>
              </td>
              
              <td style="padding:20px;width:70%;vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_FSS-1000_A_1000-Class_Dataset_for_Few-Shot_Segmentation_CVPR_2020_paper.pdf">
                <papertitle>FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation</papertitle>
                </a>
                <br>
                <strong>Xiang Li</strong>, 
                <span>Tianhan Wei</span>, 
                <span>Yau Pun Chen</span>, 
                <a href="https://scholar.google.com/citations?user=nFhLmFkAAAAJ">Yu-Wing Tai</a>, 
                <a href="http://www.cs.ust.hk/~cktang/bio-sketch-review.htm">Chi-Keung Tang</a>
                <br>
                <em>In CVPR</em>, 2020
                <br>
                <!-- <a href="oneshot/index.html">project page</a>
                / -->
                <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_FSS-1000_A_1000-Class_Dataset_for_Few-Shot_Segmentation_CVPR_2020_paper.pdf">paper</a>
                /
                <a href="https://github.com/HKUSTCV/FSS-1000">dataset and code</a>
                <p></p>
                <p>A few-shot segmentation dataset containing 1000 varied and balanced object categories with pixelwise annotation of ground-truth segmentation. </p>
              </td>
            </tr>

          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px;width:100%;vertical-align:middle;text-align:center;">
                <br>
                  <p style="font-size:small;"> This website's template is from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.</p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </div>

  <!-- Simple animation library -->
  <script src="https://unpkg.com/@lottiefiles/lottie-player@latest/dist/lottie-player.js"></script>
  
  <!-- Custom animation script -->
  <script>
    // Add smooth scrolling for all links
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        document.querySelector(this.getAttribute('href')).scrollIntoView({
          behavior: 'smooth'
        });
      });
    });

    // Add parallax effect on scroll
    window.addEventListener('scroll', function() {
      const scrollPosition = window.scrollY;
      document.body.style.backgroundPositionY = scrollPosition * 0.05 + 'px';
    });
  </script>
</body>

</html>
